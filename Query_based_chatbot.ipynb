{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhlU8X6FFXUKcMZrKx801k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajeeetsingh/document-query-chatbot/blob/main/Query_based_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cell 1: Install Required Libraries and ngrok\n",
        "-  Purpose: Installs Python libraries and ngrok to run the chatbot in Google Colab.\n",
        "-           Libraries include Streamlit (web app), sentence-transformers (NLP model),          scikit-learn (clustering), torch (model backend), nltk (text processing),          and pyngrok (public URL tunneling). This is similar to installing diffusers          and torch in the Text-to-Image project (Cell 1).\n",
        "-  Inputs: None (runs pip commands automatically).\n",
        "-  Outputs: Confirmation of installed libraries or error messages if installation fails.\n",
        "-  Context: Prepares the Colab environment for the chatbot, ensuring all dependencies          are available before processing text or running the Streamlit app"
      ],
      "metadata": {
        "id": "xT1dYwFvEjHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfzlCbGYrP4D",
        "outputId": "e82627f2-8990-4308-c1ba-1232e334af4f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Libraries and ngrok installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Purpose: Install Streamlit, sentence-transformers, NLTK, and ngrok for Colab.\n",
        "\n",
        "# Install dependencies\n",
        "!pip install streamlit sentence-transformers scikit-learn torch nltk pyngrok\n",
        "\n",
        "# Verify installations\n",
        "try:\n",
        "    import streamlit\n",
        "    import sentence_transformers\n",
        "    import sklearn\n",
        "    import torch\n",
        "    import nltk\n",
        "    import pyngrok\n",
        "    print(\"Libraries and ngrok installed successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Installation error: {e}. Please rerun this cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cell 2: Mount Google Drive and Download NLTK Resources\n",
        "-  Purpose: Mounts Google Drive to access text files (e.g., complex_sample.txt) and\n",
        "-           downloads NLTK resources (punkt, punkt_tab, wordnet) for text tokenization and query preprocessing. This mirrors mounting Drive in the Text-to-Image\n",
        "-           project (Cell 2) to save outputs.\n",
        "-  Inputs: User authentication for Google Drive (follow Colab prompt).\n",
        "-  Outputs: Mounted Drive at /content/drive and confirmation of NLTK resources.\n",
        "-  Context: Ensures text files are accessible and NLTK is ready for sentence splitting and lemmatization, critical for processing user-uploaded documents."
      ],
      "metadata": {
        "id": "PlGIyZpNE0FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Mount Google Drive and Download NLTK Resources\n",
        "# Purpose: Mount Google Drive for sample.txt/sample2.txt and download NLTK resources.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Mount Google Drive; follow authentication prompt\n",
        "\n",
        "# Download NLTK resources with verification\n",
        "import nltk\n",
        "import os\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "    print(\"NLTK resources (punkt, punkt_tab) downloaded successfully!\")\n",
        "except LookupError:\n",
        "    print(\"Failed to download NLTK resources. Retrying...\")\n",
        "    nltk.download('punkt', force=True)\n",
        "    nltk.download('punkt_tab', force=True)\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "        print(\"NLTK resources downloaded successfully!\")\n",
        "    except LookupError:\n",
        "        print(\"NLTK download failed. Please check your network and rerun.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esjS3x7OsF1b",
        "outputId": "f08ee4d7-99c3-4845-b012-1a6494a568df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "NLTK resources (punkt, punkt_tab) downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cell 3: Set Up ngrok for Public Access\n",
        "-  Purpose: Downloads and configures ngrok to expose the Streamlit app via a public\n",
        "-           URL, allowing users to access the chatbot outside Colab. Uses your ngrok\n",
        "-           authtoken for authentication. This is identical to the ngrok setup in the\n",
        "-           Text-to-Image project (Cell 9).\n",
        "-  Inputs: Your ngrok authtoken (hardcoded below; keep secure in production).\n",
        "-  Outputs: Confirmation of ngrok setup and installation.\n",
        "-  Context: Enables testing the Streamlit app in a browser, critical for user interaction and portfolio demos."
      ],
      "metadata": {
        "id": "HsPckPYkFFrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Set Up ngrok\n",
        "# Purpose: Download and configure ngrok with your authtoken.\n",
        "\n",
        "# Download and install ngrok\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "!mv ngrok /usr/local/bin/\n",
        "\n",
        "# Set up ngrok authtoken (using your provided authtoken)\n",
        "!ngrok authtoken 2wUFgUnZUHXkXPh70TnRjtiHoMg_5qpQDaz7krTTXx5Htf8q2\n",
        "\n",
        "print(\"ngrok configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyhq4w0OsVbN",
        "outputId": "4bf52244-8170-4707-c1a4-617f1fb611f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-12 23:46:19--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 99.83.220.108, 35.71.179.82, 13.248.244.96, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|99.83.220.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9395172 (9.0M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.tgz.2’\n",
            "\n",
            "ngrok-v3-stable-lin 100%[===================>]   8.96M  10.8MB/s    in 0.8s    \n",
            "\n",
            "2025-05-12 23:46:20 (10.8 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz.2’ saved [9395172/9395172]\n",
            "\n",
            "ngrok\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "ngrok configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cell 4: Save Streamlit App\n",
        "-  Purpose: Saves the Streamlit app code as app.py, defining the chatbot's logic and UI.\n",
        "-           Includes query preprocessing (lemmatization, abbreviation expansion), topic\n",
        "-           clustering (K-means), and sentence matching (all-MiniLM-L6-v2). Uses manual\n",
        "-           file writing to avoid %%writefile issues, as in the Text-to-Image project (Cell 9).\n",
        "-  Inputs: None (writes app.py to /content/).\n",
        "-  Outputs: Confirmation that app.py was saved.\n",
        "-  Context: Creates the core application, integrating NLP, clustering, and a web interface for users to upload files and query documents."
      ],
      "metadata": {
        "id": "KFeAngnzFKTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Save Streamlit App\n",
        "# Purpose: Save the Streamlit app as app.py using manual file writing, avoiding %%writefile.\n",
        "\n",
        "print(\"Saving app.py manually to avoid %%writefile issues.\")\n",
        "with open(\"/content/app.py\", \"w\") as f:\n",
        "    f.write('''\n",
        "import streamlit as st\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    st.error(\"Failed to download NLTK resources. Please refresh the app.\")\n",
        "\n",
        "# Load pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def preprocess_query(query):\n",
        "    \"\"\"\n",
        "    Preprocess the query by tokenizing, lemmatizing, and expanding abbreviations.\n",
        "    Args:\n",
        "        query (str): Raw user query.\n",
        "    Returns:\n",
        "        str: Preprocessed query.\n",
        "    \"\"\"\n",
        "    if not query.strip():\n",
        "        return query\n",
        "    # Lowercase and tokenize\n",
        "    tokens = word_tokenize(query.lower())\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Lemmatize tokens\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # Expand common abbreviations\n",
        "    expansions = {\n",
        "        \"ai\": \"artificial intelligence\",\n",
        "        \"ml\": \"machine learning\",\n",
        "        \"re\": \"renewable energy\"\n",
        "    }\n",
        "    expanded_tokens = []\n",
        "    for token in tokens:\n",
        "        expanded_tokens.append(expansions.get(token, token))\n",
        "    # Remove stop words (simple list for brevity)\n",
        "    stop_words = {\"is\", \"are\", \"what\", \"how\", \"in\", \"to\", \"for\", \"and\", \"or\"}\n",
        "    tokens = [token for token in expanded_tokens if token not in stop_words]\n",
        "    # Join tokens\n",
        "    processed_query = \" \".join(tokens)\n",
        "    return processed_query if processed_query else query\n",
        "\n",
        "def process_text_file(file_content, query, num_clusters=2, top_k=3):\n",
        "    \"\"\"\n",
        "    Process a text file's content, cluster sentences by topic, and find top-k relevant sentences.\n",
        "    Args:\n",
        "        file_content (str): Text content of the uploaded file.\n",
        "        query (str): User query.\n",
        "        num_clusters (int): Number of topic clusters (default: 2).\n",
        "        top_k (int): Number of top sentences to return (default: 3).\n",
        "    Returns:\n",
        "        tuple: (results, all_sentences, all_scores, cluster_labels) or (error_message, None, None, None).\n",
        "    \"\"\"\n",
        "    if not file_content.strip():\n",
        "        return \"Error: File is empty!\", None, None, None\n",
        "    if not query.strip():\n",
        "        return \"Error: Please enter a query!\", None, None, None\n",
        "    try:\n",
        "        sentences = nltk.sent_tokenize(file_content)\n",
        "    except LookupError as e:\n",
        "        return f\"Tokenization error: {e}.\", None, None, None\n",
        "    if not sentences:\n",
        "        return \"Error: No sentences found in the file!\", None, None, None\n",
        "    if len(sentences) < 2:\n",
        "        return \"Error: File must contain at least two sentences.\", None, None, None\n",
        "\n",
        "    # Preprocess query\n",
        "    processed_query = preprocess_query(query)\n",
        "\n",
        "    # Generate sentence and query embeddings\n",
        "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    query_embedding = model.encode(processed_query, convert_to_tensor=True)\n",
        "\n",
        "    # Cluster sentences by topic\n",
        "    num_clusters = min(num_clusters, len(sentences))\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(sentence_embeddings.cpu().numpy())\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    cos_scores = util.cos_sim(query_embedding, sentence_embeddings)[0].cpu().numpy()\n",
        "\n",
        "    # Find the most relevant cluster\n",
        "    cluster_scores = []\n",
        "    for cluster_id in range(num_clusters):\n",
        "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            cluster_score = np.mean(cos_scores[cluster_indices])\n",
        "            cluster_scores.append((cluster_id, cluster_score))\n",
        "    if not cluster_scores:\n",
        "        return \"Error: No valid clusters found.\", None, None, None\n",
        "    relevant_cluster = max(cluster_scores, key=lambda x: x[1])[0]\n",
        "\n",
        "    # Get top-k sentences from the relevant cluster\n",
        "    cluster_indices = np.where(cluster_labels == relevant_cluster)[0]\n",
        "    if len(cluster_indices) == 0:\n",
        "        return \"Error: No sentences in the relevant cluster.\", None, None, None\n",
        "    cluster_scores = cos_scores[cluster_indices]\n",
        "    cluster_sentences = [sentences[i] for i in cluster_indices]\n",
        "    top_k_indices = np.argsort(cluster_scores)[::-1][:min(top_k, len(cluster_scores))]\n",
        "    results = []\n",
        "    for idx in top_k_indices:\n",
        "        sentence = cluster_sentences[idx]\n",
        "        score = cluster_scores[idx]\n",
        "        explanation = f\"This sentence was selected because it closely matches the processed query '{processed_query}' (original: '{query}') with a similarity score of {score:.4f} and belongs to topic cluster {relevant_cluster + 1}, which is most relevant to your query.\"\n",
        "        results.append({\"sentence\": sentence, \"score\": score, \"topic\": f\"Topic {relevant_cluster + 1}\", \"explanation\": explanation})\n",
        "\n",
        "    return results, sentences, cos_scores.tolist(), cluster_labels.tolist()\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Document Query Chatbot\")\n",
        "st.markdown(\"\"\"\n",
        "Upload a text file (.txt, 100–1000 words) containing multiple topics (e.g., AI and renewable energy).\n",
        "Enter a query to find the most relevant sentences, grouped by topic.\n",
        "- **Example file**: A document discussing AI and renewable energy.\n",
        "- **Example query**: \"What's AI?\" or \"What is renewable energy?\"\n",
        "\"\"\")\n",
        "\n",
        "# File uploader\n",
        "uploaded_file = st.file_uploader(\"Upload a text file\", type=\"txt\")\n",
        "\n",
        "# Query input\n",
        "query = st.text_input(\"Enter your query\", placeholder=\"e.g., What's AI?\")\n",
        "\n",
        "# Number of clusters\n",
        "num_clusters = st.slider(\"Number of topic clusters\", min_value=2, max_value=5, value=2)\n",
        "\n",
        "# Number of sentences to return\n",
        "top_k = st.slider(\"Number of sentences to return\", min_value=1, max_value=5, value=3)\n",
        "\n",
        "# Process and display results\n",
        "if uploaded_file and query:\n",
        "    with st.spinner(\"Processing query and clustering sentences...\"):\n",
        "        file_content = uploaded_file.read().decode(\"utf-8\")\n",
        "        results, all_sentences, all_scores, cluster_labels = process_text_file(file_content, query, num_clusters, top_k)\n",
        "        if isinstance(results, list) and results:\n",
        "            st.markdown(\"**Top Matching Sentences**\")\n",
        "            for result in results:\n",
        "                st.markdown(f\"- **Sentence**: {result['sentence']}\")\n",
        "                st.markdown(f\"  - **Topic**: {result['topic']}\")\n",
        "                st.markdown(f\"  - **Score**: {result['score']:.4f}\")\n",
        "                st.markdown(f\"  - **Explanation**: {result['explanation']}\")\n",
        "            with st.expander(\"View all sentences and their topics\"):\n",
        "                for sent, score, cluster in zip(all_sentences, all_scores, cluster_labels):\n",
        "                    st.write(f\"- Sentence: {sent} (Score: {score:.4f}, Topic: {cluster + 1})\")\n",
        "        else:\n",
        "            st.error(results)\n",
        "elif uploaded_file and not query:\n",
        "    st.warning(\"Please enter a query to proceed.\")\n",
        "elif query and not uploaded_file:\n",
        "    st.warning(\"Please upload a text file to proceed.\")\n",
        "''')\n",
        "print(\"app.py saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KUa7mUKsZXi",
        "outputId": "1a4148dd-0784-4105-c72d-0bbdc0d2a665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving app.py manually to avoid %%writefile issues.\n",
            "app.py saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cell 5: Run Streamlit App with ngrok\n",
        "- Purpose: Launches the Streamlit app (app.py) in Colab and exposes it via a public ngrok URL for browser access. Includes a clean shutdown mechanism for interrupting the process. This mirrors the Streamlit/ngrok setup in the Text-to-Image project (Cell 9).\n",
        "- Inputs: app.py (from Cell 4) and ngrok configuration (from Cell 3).\n",
        "- Outputs: Public ngrok URL (e.g., https://abc123.ngrok.io) to access the app.\n",
        "- Context: Allows users to interact with the chatbot’s web interface, testing file uploads and queries in a real-world setting."
      ],
      "metadata": {
        "id": "59PAhOEIGC4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Run Streamlit App with ngrok\n",
        "# Purpose: Run app.py in Colab and expose it via a public ngrok URL.\n",
        "\n",
        "import subprocess\n",
        "import signal\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app running at: {public_url}\")\n",
        "\n",
        "# Start Streamlit server\n",
        "streamlit_cmd = [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.fileWatcherType\", \"none\"]\n",
        "streamlit_proc = subprocess.Popen(streamlit_cmd)\n",
        "\n",
        "# Handle shutdown (matches Text-to-Image Cell 9)\n",
        "def signal_handler(sig, frame):\n",
        "    print(\"Shutting down Streamlit and ngrok...\")\n",
        "    streamlit_proc.terminate()\n",
        "    ngrok.kill()\n",
        "    print(\"Shutdown complete.\")\n",
        "    os._exit(0)\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "# Keep the cell running\n",
        "try:\n",
        "    streamlit_proc.wait()\n",
        "except KeyboardInterrupt:\n",
        "    signal_handler(None, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0frPXeasgoy",
        "outputId": "e6241f3b-6585-4849-aaa6-69137493fe59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app running at: NgrokTunnel: \"https://3303-34-125-114-107.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}